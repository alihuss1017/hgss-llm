{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('GITHUB_TOKEN')\n",
        "!git clone https://alihuss1017:{github_token}@github.com/alihuss1017/hgss-llm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC1T_uWV2BS0",
        "outputId": "0b7a1122-8164-450e-f386-0a7433c22ce5"
      },
      "id": "gC1T_uWV2BS0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hgss-llm'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 168 (delta 43), reused 120 (delta 42), pack-reused 44 (from 1)\u001b[K\n",
            "Receiving objects: 100% (168/168), 6.23 MiB | 31.40 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd hgss-llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMQy5OmA2u3t",
        "outputId": "092c0780-41b3-4223-c941-074500b106c5"
      },
      "id": "lMQy5OmA2u3t",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hgss-llm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout -b RAG-pipeline origin/RAG-pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3mVxFOj2qqN",
        "outputId": "04921e1b-28b9-4330-dce5-5595be63add9"
      },
      "id": "o3mVxFOj2qqN",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'RAG-pipeline' set up to track remote branch 'RAG-pipeline' from 'origin'.\n",
            "Switched to a new branch 'RAG-pipeline'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RDqLuhZ0dh9l",
      "metadata": {
        "id": "RDqLuhZ0dh9l"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Ouy6ori5jlMs",
      "metadata": {
        "id": "Ouy6ori5jlMs"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import pickle\n",
        "\n",
        "# Load FAISS index\n",
        "index = faiss.read_index(\"data/RAG/faiss_index.index\")\n",
        "\n",
        "# Load metadata (text chunks and their metadata)\n",
        "with open(\"data/RAG/metadata.pkl\", \"rb\") as f:\n",
        "    text_chunks, metadata_chunks = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "Bg3-KqkBCWBs"
      },
      "id": "Bg3-KqkBCWBs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "Sht936l7j7RZ",
      "metadata": {
        "id": "Sht936l7j7RZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def retrieve_relevant_chunks(query, top_k=3):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
        "    return [(text_chunks[i], metadata_chunks[i]) for i in indices[0]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n"
      ],
      "metadata": {
        "id": "fXyRu6iGClfz",
        "outputId": "16c4f2a7-2c81-4d9b-f8a4-f0a06b0c316d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fXyRu6iGClfz",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, top_k=3, max_new_tokens=150):\n",
        "    # 1. Retrieve context\n",
        "    retrieved_chunks = retrieve_relevant_chunks(query, top_k=top_k)\n",
        "    context = \"\\n\\n\".join([chunk for chunk, _ in retrieved_chunks])\n",
        "\n",
        "    # 2. Create chat-style messages\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert on PokÃ©mon HeartGold and SoulSilver. Use only the provided context to answer accurately.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Answer the following question using the context below:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\"\"\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # 3. Apply chat template\n",
        "    prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True  # Appends the assistant's turn (start of answer)\n",
        "    )\n",
        "\n",
        "    # 4. Generate\n",
        "    output = pipe(prompt, max_new_tokens=max_new_tokens)\n",
        "    return output[0][\"generated_text\"][len(prompt):].strip()\n"
      ],
      "metadata": {
        "id": "exq_EIfNIY4d"
      },
      "id": "exq_EIfNIY4d",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "XJYALCYYlf7p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJYALCYYlf7p",
        "outputId": "e62ef5f2-3412-4eb8-eaa7-70c9ecdcbc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ To answer the question, Falkner's team in Violet City is [{'pokemon': 'Pidgey', 'level': 9}, {'pokemon': 'Pidgeotto', 'level': 13}].\n"
          ]
        }
      ],
      "source": [
        "query = \"I'm about to challenge Falkner in Violet City, what is his team?\"\n",
        "print(\"ðŸ’¬\", generate_answer(query))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}