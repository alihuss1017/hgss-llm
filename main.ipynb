{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e7fabec",
      "metadata": {
        "id": "0e7fabec"
      },
      "source": [
        "## Setup: Cloning Repository and Installing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "gC1T_uWV2BS0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC1T_uWV2BS0",
        "outputId": "5b37ba0d-5927-46f3-9a67-0f9cbe52b078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hgss-llm'...\n",
            "remote: Enumerating objects: 202, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 202 (delta 52), reused 146 (delta 46), pack-reused 44 (from 1)\u001b[K\n",
            "Receiving objects: 100% (202/202), 6.24 MiB | 22.82 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('GITHUB_TOKEN')\n",
        "!git clone https://alihuss1017:{github_token}@github.com/alihuss1017/hgss-llm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "lMQy5OmA2u3t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMQy5OmA2u3t",
        "outputId": "7c70e83a-a0cb-4cf2-f22c-0704d9ff1ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hgss-llm\n"
          ]
        }
      ],
      "source": [
        "cd hgss-llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "o3mVxFOj2qqN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3mVxFOj2qqN",
        "outputId": "d1bd650b-5f83-45d2-be69-740548914529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'RAG-pipeline' set up to track remote branch 'RAG-pipeline' from 'origin'.\n",
            "Switched to a new branch 'RAG-pipeline'\n"
          ]
        }
      ],
      "source": [
        "!git checkout -b RAG-pipeline origin/RAG-pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RDqLuhZ0dh9l",
      "metadata": {
        "id": "RDqLuhZ0dh9l"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7db9bde",
      "metadata": {
        "id": "f7db9bde"
      },
      "source": [
        "## Loading Precomputed Vector Store and Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Ouy6ori5jlMs",
      "metadata": {
        "id": "Ouy6ori5jlMs"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import pickle\n",
        "\n",
        "index = faiss.read_index(\"data/RAG/faiss_index.index\")\n",
        "\n",
        "with open(\"data/RAG/metadata.pkl\", \"rb\") as f:\n",
        "    text_chunks, metadata_chunks = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ababbfc6",
      "metadata": {
        "id": "ababbfc6"
      },
      "source": [
        "## Loading Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bg3-KqkBCWBs",
      "metadata": {
        "id": "Bg3-KqkBCWBs"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "799ca85a",
      "metadata": {
        "id": "799ca85a"
      },
      "source": [
        "## Defining Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "Sht936l7j7RZ",
      "metadata": {
        "id": "Sht936l7j7RZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def retrieve_relevant_chunks(query, top_k=3):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    _, indices = index.search(np.array(query_embedding), top_k)\n",
        "    return [(text_chunks[i], metadata_chunks[i]) for i in indices[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e426a6",
      "metadata": {
        "id": "98e426a6"
      },
      "source": [
        "## Loading Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXyRu6iGClfz",
      "metadata": {
        "id": "fXyRu6iGClfz"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1eff89",
      "metadata": {
        "id": "ba1eff89"
      },
      "source": [
        "## Defining Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "exq_EIfNIY4d",
      "metadata": {
        "id": "exq_EIfNIY4d"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query, top_k=3, max_new_tokens=150):\n",
        "    retrieved_chunks = retrieve_relevant_chunks(query, top_k=top_k)\n",
        "    context = \"\\n\\n\".join([chunk for chunk, _ in retrieved_chunks])\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert on Pok√©mon HeartGold and SoulSilver. Use only the provided context to answer accurately.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Answer the following question using the context below:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\"\"\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "\n",
        "    prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # 4. Generate\n",
        "    output = pipe(prompt, max_new_tokens=max_new_tokens)\n",
        "    return output[0][\"generated_text\"][len(prompt):].strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Answers using Evaluation Q&A Dataset"
      ],
      "metadata": {
        "id": "Rmf5-qHgp78J"
      },
      "id": "Rmf5-qHgp78J"
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "with jsonlines.open(\"data/eval/hgss-QA-base.jsonl\") as f:\n",
        "    eval_data = list(f)\n",
        "\n",
        "\n",
        "for item in eval_data:\n",
        "    item[\"generated_answer\"] = generate_answer(item[\"question\"])\n",
        "\n",
        "with jsonlines.open(\"data/eval/hgss-QA-complete.jsonl\", mode = \"w\") as f:\n",
        "    f.write_all(eval_data)\n"
      ],
      "metadata": {
        "id": "FYYg8fhKqCgt"
      },
      "id": "FYYg8fhKqCgt",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e972ba4a",
      "metadata": {
        "id": "e972ba4a"
      },
      "source": [
        "## Computing Cosine Similarity on Evaluation Q&A Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "AcXKNyz5QrfK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcXKNyz5QrfK",
        "outputId": "c464c44e-a667-4be3-e339-a067aa6ec977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py:196: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cosine similarity: 0.47737744450569153\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "cosine_scores = []\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', use_auth_token=hf_token)\n",
        "\n",
        "with open('data/eval/hgss-QA-complete.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        gold = data['gold_answer']\n",
        "        generated = data['generated_answer']\n",
        "\n",
        "        embeddings = model.encode([gold, generated])\n",
        "        score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "        cosine_scores.append(score)\n",
        "\n",
        "\n",
        "print(f'Average cosine similarity: {sum(cosine_scores) / len(cosine_scores)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d82c21e0",
      "metadata": {
        "id": "d82c21e0"
      },
      "source": [
        "## Sample Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XJYALCYYlf7p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJYALCYYlf7p",
        "outputId": "e62ef5f2-3412-4eb8-eaa7-70c9ecdcbc83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí¨ To answer the question, Falkner's team in Violet City is [{'pokemon': 'Pidgey', 'level': 9}, {'pokemon': 'Pidgeotto', 'level': 13}].\n"
          ]
        }
      ],
      "source": [
        "query = \"I'm about to challenge Falkner in Violet City, what is his team?\"\n",
        "print(\"üí¨\", generate_answer(query))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}