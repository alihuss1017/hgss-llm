{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('GITHUB_TOKEN')\n",
        "!git clone https://alihuss1017:{github_token}@github.com/alihuss1017/hgss-llm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC1T_uWV2BS0",
        "outputId": "29a0e610-576b-4940-eeb2-bef3791db412"
      },
      "id": "gC1T_uWV2BS0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hgss-llm'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 187 (delta 47), reused 136 (delta 45), pack-reused 44 (from 1)\u001b[K\n",
            "Receiving objects: 100% (187/187), 6.24 MiB | 17.89 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd hgss-llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMQy5OmA2u3t",
        "outputId": "11608032-db9c-4f9c-f8d5-5c0f5d6a899b"
      },
      "id": "lMQy5OmA2u3t",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hgss-llm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout -b RAG-pipeline origin/RAG-pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3mVxFOj2qqN",
        "outputId": "80cd4be7-667c-42d8-95f3-299023c9adb5"
      },
      "id": "o3mVxFOj2qqN",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'RAG-pipeline' set up to track remote branch 'RAG-pipeline' from 'origin'.\n",
            "Switched to a new branch 'RAG-pipeline'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RDqLuhZ0dh9l",
      "metadata": {
        "id": "RDqLuhZ0dh9l"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Ouy6ori5jlMs",
      "metadata": {
        "id": "Ouy6ori5jlMs"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import pickle\n",
        "\n",
        "# Load FAISS index\n",
        "index = faiss.read_index(\"data/RAG/faiss_index.index\")\n",
        "\n",
        "# Load metadata (text chunks and their metadata)\n",
        "with open(\"data/RAG/metadata.pkl\", \"rb\") as f:\n",
        "    text_chunks, metadata_chunks = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "Bg3-KqkBCWBs"
      },
      "id": "Bg3-KqkBCWBs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "Sht936l7j7RZ",
      "metadata": {
        "id": "Sht936l7j7RZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def retrieve_relevant_chunks(query, top_k=3):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
        "    return [(text_chunks[i], metadata_chunks[i]) for i in indices[0]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n"
      ],
      "metadata": {
        "id": "fXyRu6iGClfz"
      },
      "id": "fXyRu6iGClfz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, top_k=3, max_new_tokens=150):\n",
        "    # 1. Retrieve context\n",
        "    retrieved_chunks = retrieve_relevant_chunks(query, top_k=top_k)\n",
        "    context = \"\\n\\n\".join([chunk for chunk, _ in retrieved_chunks])\n",
        "\n",
        "    # 2. Create chat-style messages\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert on PokÃ©mon HeartGold and SoulSilver. Use only the provided context to answer accurately.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Answer the following question using the context below:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\"\"\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # 3. Apply chat template\n",
        "    prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True  # Appends the assistant's turn (start of answer)\n",
        "    )\n",
        "\n",
        "    # 4. Generate\n",
        "    output = pipe(prompt, max_new_tokens=max_new_tokens)\n",
        "    return output[0][\"generated_text\"][len(prompt):].strip()\n"
      ],
      "metadata": {
        "id": "exq_EIfNIY4d"
      },
      "id": "exq_EIfNIY4d",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "cosine_scores = []\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', use_auth_token=hf_token)\n",
        "\n",
        "with open('data/eval/hgss-QA-complete.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        gold = data['gold_answer']\n",
        "        generated = data['generated_answer']\n",
        "\n",
        "        embeddings = model.encode([gold, generated])\n",
        "        score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "        cosine_scores.append(score)\n",
        "\n",
        "\n",
        "print(f'Average cosine similarity: {sum(cosine_scores) / len(cosine_scores)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcXKNyz5QrfK",
        "outputId": "27a3b2bf-1b18-4746-ef81-05fd29c20545"
      },
      "id": "AcXKNyz5QrfK",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cosine similarity: 0.5192123055458069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XJYALCYYlf7p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJYALCYYlf7p",
        "outputId": "e62ef5f2-3412-4eb8-eaa7-70c9ecdcbc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ To answer the question, Falkner's team in Violet City is [{'pokemon': 'Pidgey', 'level': 9}, {'pokemon': 'Pidgeotto', 'level': 13}].\n"
          ]
        }
      ],
      "source": [
        "query = \"I'm about to challenge Falkner in Violet City, what is his team?\"\n",
        "print(\"ðŸ’¬\", generate_answer(query))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}